version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.10.2
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    MLFLOW_TRACKING_URI: http://mlflow:5000
    FRED_API_KEY: ${FRED_API_KEY}
    # Pointing to the location accessible by the airflow user
    GOOGLE_APPLICATION_CREDENTIALS: /opt/airflow/gcloud/application_default_credentials.json
    GOOGLE_CLOUD_PROJECT: macroflow-486515
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock
    # Map your local keys to the accessible opt path
    - /Users/zacharymyrick/.config/gcloud:/opt/airflow/gcloud:ro
    # Corrected data mount to ensure DuckDB is shared between host and worker
    - ./data:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:0"
  group_add:
    - "${DOCKER_GID:-999}" 

services:
  # --- DATABASE LAYER ---
  postgres:
    image: postgres:13
    container_name: airflow_db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- ORCHESTRATION LAYER ---
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    container_name: airflow_webserver
    ports:
      - "8085:8080"
    depends_on:
      postgres:
        condition: service_healthy

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    container_name: airflow_scheduler
    depends_on:
      postgres:
        condition: service_healthy

  # --- CI/CD & TRACKING ---
  jenkins:
    image: jenkins/jenkins:lts
    container_name: jenkins
    privileged: true
    user: root
    ports:
      - "8080:8080"
      - "50000:50000"
    volumes:
      - jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.2
    container_name: mlflow_server
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlruns
    command: mlflow server --host 0.0.0.0 --port 5000

  # --- DATA ENGINE (For building the image) ---
  macro-engine:
    build: .
    image: macro-pipeline-engine-macro-engine:latest
    working_dir: /app/dbt_macro
    volumes:
      - .:/app
      - /Users/zacharymyrick/.config/gcloud:/root/.config/gcloud:ro
    environment:
      - GOOGLE_APPLICATION_CREDENTIALS=/root/.config/gcloud/application_default_credentials.json
      - GOOGLE_CLOUD_PROJECT=macroflow-486515
      - DBT_PROFILES_DIR=/app/dbt_macro
    profiles: ["donotrun"]

volumes:
  postgres_data:  # Critical: Keeps your connections/variables alive
  mlflow_data:
  jenkins_home: